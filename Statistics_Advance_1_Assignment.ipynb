{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1)\n",
        "\n",
        "Solution:\n",
        "\n",
        "The F-distribution is a probability distribution that arises frequantly in statistics, particularly in analysis of variance(ANOVA), regression analysis, and hypothesis testing. It is the ratio of two independent chi-square distribution divided by their respective degrees of freedom. Some key properties of F-distribution:\n",
        "\n",
        "1) Non-Negative Values:\n",
        "\n",
        "The F-distribution is defined only for non-negative values because it is the ratio of squared terms.\n",
        "\n",
        "2) Asymmetry:\n",
        "\n",
        "* The F-distribution is positively skewed (right-skewed)\n",
        "* The degree of skewness decreases as the degrees of freedom for the numerator and denominator increases, making the distribution more symmetric.\n",
        "\n",
        "3) Shape and Parameter:\n",
        "\n",
        "* The shape of the F-distribution depends on two parameters:\n",
        "\n",
        " * d1: Degrees of freedom of the numerator.\n",
        "\n",
        " * d2: Degrees of freedom of denomenator.\n",
        "\n",
        "* Larger values of d1 and d2 result the F-distribution approaching a normal distribution.\n",
        "\n",
        "4) Mean of the distribution:\n",
        "\n",
        "* The mean exists only if d2>2 and is given by:\n",
        "\n",
        "Mean = d2/d2-2\n",
        "\n",
        "5) Range:\n",
        "\n",
        "The range of the F-disribution is (0,∞)\n",
        "\n",
        "6) Additivity:\n",
        "\n",
        "If Xₗ and X2 are independent F-distributed random variables with the some denominator degrees of freedom, the sum of these variables is also F-distributes.\n",
        "\n",
        "7) Relationship to other Distributions:\n",
        "\n",
        "* Chi-Square Distribution: The F-distribution is the ratio of two independent chi-square variables divided by their respective degrees of freedom.\n",
        "\n",
        "* T- Distribution: The square of a t-distributed variable with n degrees of freedom divided by n follows on F-distribution.\n",
        "\n",
        "8) Applications:\n",
        "\n",
        "* ANOVA: Used to test the hypothesis that several population means are equal.\n",
        "\n",
        "* Regression Analysis: Used to test the overall significance of a significance model.\n",
        "\n",
        "* Comparision of Variances: Used to test if two populations have equal variances.\n",
        "\n",
        "By combining these properties, the F-distribution serves as a cornerstone for many inferential statistics methods."
      ],
      "metadata": {
        "id": "jocPQgeF9WX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2)\n",
        "\n",
        "Solution:\n",
        "\n",
        "F-distribution is used in several statistical tests where comparing variances or assessing ratios of variances required. It is appropriate for these tests because it describes the ratios of two independent chi-squared distributions normalised by their respective degrees of freedom.\n",
        "\n",
        "1) Analysis of variance (ANOVA):\n",
        "\n",
        "* Purpose: To comapre the means of three or more groups by analyzing the ratio of within-group variance.\n",
        "\n",
        "* Why F-Distribution is appropriate: In ANOVA, the F-statistic is calculated as the ratio of the variance due to group differences (mean square between groups). Since these variances follows chi-squared distributions when scaled, their ratio follows the F-distribution.\n",
        "\n",
        "2) Regression Analysis:\n",
        "\n",
        "Purpose: To test the overall significance of a regression model (e.g., whether the predictors collectively explain a significant amount of variance in the dependent variable).\n",
        "\n",
        "Why F-distribution is appropriate: The F-statistic in regression is calculated by comparing the mean square due to regression (explained variance) to the mean square error (unexplained variance). These variances also follow chi-squared distributions, making the F-distribution suitable for their ratio.\n",
        "\n",
        "3) Test For Equality of two Variances:\n",
        "\n",
        "Purpose: To compare the variances of two independent samples.\n",
        "\n",
        "Why F-distribution is appropraited: The F-statistic here is the ratio of the two sample variances. Since sample variances are propotional to chi-squared distributions, their ratio follows on F-distribution under the null hypothesis.\n",
        "\n",
        "4) MANOVA (Multivariate Analysis of variance):\n",
        "\n",
        "Purpose: To test differences in means across multiple depedent variable simultaneously.\n",
        "\n",
        "Why F-distribution is appropriate: MANOVA extends ANOVA to multiple dependent variables, and the F-distribution is used in testing the significance of these multivariate effects.\n",
        "\n",
        "5) Genearal Linear models:\n",
        "\n",
        "Purpose: To evaluate hypothesis about linear relationships in a variety of frameworks (e.g., comparing nested models).\n",
        "\n",
        "Why F-distribution is appropraite: When comparing models, the ratio of exlained variance to the residual variance is assessed, which aligns with the F-distribution framework.\n",
        "\n",
        "Summary: The F-distribution is appropriate for tests where the focus is on comapring variances or variance-based matrics because it accurately models the ratio of variances under the null-hypothesis.\n",
        "\n"
      ],
      "metadata": {
        "id": "NjMM3j0THaif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3)\n",
        "\n",
        "Solution:\n",
        "\n",
        "The key assumptions for conducting an f-test to compare the variances of two populations are as follow:\n",
        "\n",
        "1) Independent samples:\n",
        "\n",
        "The two samples must be independent of each other. This means that the data from one population should not influence or be related to the data from the other population.\n",
        "\n",
        "2) Normality:\n",
        "\n",
        "The data in both populations should be approximately normaly distributed. The F-test is sensitive to departuress from normality. So, this asssumption is crucial.\n",
        "\n",
        "3) Random Sampling:\n",
        "\n",
        "The samples must be randomly selected from their respective populations to ensure representativeness.\n",
        "\n",
        "4) Ratio of variances:\n",
        "\n",
        "The F-test assumes taht the ratio of variances follows an F-distribution under the null hypothesis.\n",
        "\n",
        "* If the normality assumption is violated , alternative methods like the Levene's test or Bartlett's test can be used, which are more robust to non-normal data.\n",
        "\n",
        "* Always inspect the data (e.g. Using histograms or normality tests) before applying the F-test to ensure the assumptions are met."
      ],
      "metadata": {
        "id": "jPWt_pNX87jX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4)\n",
        "\n",
        "Solution:\n",
        "\n",
        "The purpose of ANOVA (Analysis of Variance) is to determine whether there are statistically significant differences in the means of three or more groups. In contrast, a t-test is used to compare the means of two groups.\n",
        "\n",
        "Differences between ANOVA and t-test:\n",
        "\n",
        "1) Purpose:\n",
        "\n",
        "* t-test: Compares the mean of two groups to determine if they are significantly different.\n",
        "\n",
        "* ANOVA: compares the means of three or more groups simultaneously to determine if there is atleast one significant difference between group means.\n",
        "\n",
        "2) Number of groups:\n",
        "\n",
        "* t-test: Limited to two groups (e.g., Grooup A vs. Group B).\n",
        "\n",
        "* ANOVA: can handle three or more groups ( e.g., Group A vs. Group B vs Group C).\n",
        "\n",
        "3) Hypothesis:\n",
        "\n",
        "* t-test: Tests a single hypothesis:\n",
        "\n",
        " * Null hypothesis: This means of the two groups are equal( μ1 = μ2).\n",
        "\n",
        " * Alternative Hypothesis: The means of the two groups are not equal.\n",
        "\n",
        "* ANOVA: Tests a single null hypothesis:\n",
        "\n",
        " * Null hypothesis: All group means are equal.\n",
        "\n",
        " Alternative hypothesis: At least one group mean is significantly different.\n",
        "\n",
        "4) Test Statistic:\n",
        "\n",
        " * t-test: Produces a t-ststistic, which compares the differences between group means relative to variability within the data.\n",
        "\n",
        " * ANOVA: Produces an F-Statistic, which is the ratio of:\n",
        "\n",
        "     * Variance between groups (differences among group means).\n",
        "\n",
        "     * Variance within groups (variability within each group).\n",
        "\n",
        "5) Applicability:\n",
        "\n",
        "* t-test: cannot analyze more than two groups simultaneously. For multiple groups, repeated t-tests would be required, which increases the risk of Type 1 error (false positives).\n",
        "\n",
        "* ANOVA: Suitable for analyzing three or more groups simultaneously without increasing the risk of Type 1 error.\n",
        "\n",
        "6) Output:\n",
        "\n",
        "* t-test: Provides a t-value and p-value to assess whether the means of two groups differ significantly.\n",
        "\n",
        "* ANOVA: Provides an F-statistic and p-value. If the p-value is significant, a post hoc test (e.g., Tukey's HSD) is needed to dentify which groups differ.\n",
        "\n",
        "7) Example Scenarios:\n",
        "\n",
        "* t-test:\n",
        "\n",
        " * Comaparing the average test scores of two classes (Class A vs. Class B).\n",
        "\n",
        "* ANOVA:\n",
        "\n",
        "  * Comparing the average test scores of three or more classes (Class A vs. Class B vs. Class C).\n",
        "\n",
        "8) Post Hoc analysis:\n",
        "\n",
        "* t-test: No post Hoc analysis is required because it only compares two groups.\n",
        "\n",
        "* ANOVA: Post Hoc tests (e.g., Tukey's Bonferrani) are needed if ANOVA shows significant differences, to determine which specific groups differ.\n"
      ],
      "metadata": {
        "id": "1ouM_2iABYZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5)\n",
        "\n",
        "Solution:\n",
        "\n",
        "You would use a one-way ANOVA  instead of multiple t-tests when comparing two or more than two groups because of following reasons:\n",
        "\n",
        "1) Risk of Type 1 Error\n",
        "\n",
        "* Issue with multiple t-tests:\n",
        "\n",
        "When performing multiple t-tests to compare each pair of groups , the probability of committing a Type 1 Error( false positive) increases with each additional test. This means there is a higher chance of incorrectly concluding that there is a significant difference when isn't one.\n",
        "\n",
        "* Advantage of ANOVA:\n",
        "\n",
        "A one-way ANOVA tests all group means simultaneously, keeping the significance level controlled at 0.05. This avoids the inflation of Type 1 error.\n",
        "\n",
        "2) Efficiency and Simplicity:\n",
        "\n",
        "* Issues with t-tests:\n",
        "Performing multiple t-tests can be time-consuming and cumbersome as the number of groups increases. For n groups, you would need n(n-1)/2 t-tests to compare all pairs of groups.\n",
        "\n",
        " * Example:\n",
        "\n",
        "   * 3 groups- 3 t-tests\n",
        "   * 4 groups- 6 t-tests\n",
        "   * 5 groups- 10 t-tests\n",
        "\n",
        "* Advantage of ANOVA:\n",
        "\n",
        "A one-way ANOVA compares all group means in a single test, making it more efficient and easier to perform and interpret.\n",
        "\n",
        "3) Interpretation of REsults:\n",
        "\n",
        "* Issue with t-tests:\n",
        "\n",
        "Multiple t-tests provide pairwise comparisions, but they do not indicate whether the overall variability among groups is significant. They focus only on individual comparisions, which can miss the bigger picture.\n",
        "\n",
        "* Advantage of ANOVA:\n",
        "\n",
        "ANOVA evaluates whether there is any significant difference among the group means as a whole. If the ANOVA result is significant, post hoc tests (e.g., Tukey's HSD) can be used to identify specific group differences.\n",
        "\n",
        "4) Statistical Assumptions:\n",
        "\n",
        "Both one-way ANOVA and t-tests share similar assumptions:\n",
        "\n",
        " * Data are normally distributed.\n",
        " * Variances across groups are equal (homogeneity of variance).\n",
        " * Observations are independent.\n",
        "\n",
        " However, with multiple-tests, the assumptions are applied separately for each test, which can lead to inconsistencies. ANOVA applies these assumptions uniformally across all groups, ensuring consistency in the analysis."
      ],
      "metadata": {
        "id": "Djb0e6CsUxPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6)\n",
        "\n",
        "Solution:\n",
        "\n",
        "In analysis of variance (ANOVA), the total variability in the data is partitioned into two main components:\n",
        "\n",
        "1) Between-group variance:\n",
        "\n",
        "Variability due to differences between the group means.\n",
        "\n",
        "2) Within-group variance:\n",
        "\n",
        "Variability within each group, caused by individual differences or random error.\n",
        "\n",
        "This partitioning forms the basis for calculating F-statistic, which is used to test if there are significant differences among group means.\n",
        "\n",
        "1) Total Variance (SST):\n",
        "\n",
        "* The total variance measures the overall variability in the data, regardless of group membership.\n",
        "\n",
        "2) Between-group Variance (SSB):\n",
        "\n",
        "* The between-group variance measures how much the group means differ from the overall mean.\n",
        "\n",
        "* It reflects the variability explained by the group differences.\n",
        "\n",
        "3) Within-group Variance (SSW):\n",
        "\n",
        "* The within-group variance measures how much individual observations vary within each group.\n",
        "\n",
        "* It reflects variability not explained by the group differences (i.e. random noise or individual differences).\n",
        "\n",
        "4) Relationship Between Variance:\n",
        "\n",
        "The total variance is the sum of between-group variance and within-group variance:\n",
        "\n",
        "SST = SSB + SSW\n",
        "\n",
        "5) Contribution to the F-statistic:\n",
        "\n",
        "The F-statistic compares the ratio of between-group variance to within-group variance:\n",
        "\n",
        "         F = Mean Square Between(MSB)/ Mean Square Within(MSW)\n",
        "\n",
        "\n",
        "Mean Square Calculations:\n",
        "\n",
        "1) Mean Square Between (MSB):\n",
        "\n",
        "          MSB = SSB/ k - I\n",
        "\n",
        "2) Mean Square Within (MSW):\n",
        "\n",
        "         MSW = SSW/ n-k\n",
        "\n",
        "Interpretation of the F-statistic:\n",
        "\n",
        "* A large F-value indicates that the between-group variance is much larger than the witin-group variance, suggesting significant differences between group means.\n",
        "\n",
        "* A small F- value suggests that the most of the variability is within groups, implying no significant differences between group means.\n",
        "\n"
      ],
      "metadata": {
        "id": "aH6S04Eot5_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7)\n",
        "\n",
        "Solution:\n",
        "\n",
        "The classical (Frequentist) approach to ANOVA and the Bayesian approach to ANOVA differ fundamentally in how they handle uncertainity, parameter estimation and hypothesis testing.\n",
        "\n",
        "1) Treatment of Uncertainity:\n",
        "\n",
        "* Frequentist Approach:\n",
        "\n",
        "  * Assumes that parameters (e.g., group means, variances) are fixed but unknown.\n",
        "\n",
        "  * Uncertaity arises safely from the sampling process. Confidence intervals and p-values are used to quantify uncertainty about sample-based interferences.\n",
        "\n",
        "  * Results are interpreted as long-run frequancies.\n",
        "\n",
        "* Bayesian Approach:\n",
        "\n",
        "  * Treats parameters (e.g., group means, variance) as random variables with probability distributions.\n",
        "\n",
        "  * Uncertainty is expressed directly in terms of probabilities (e.g., \" The probability that the group A's mean is larger than group B's mean is 80%).\n",
        "\n",
        "  * Prior distributions and observed data are combined using Baye's theorm to update uncertainty.\n",
        "\n",
        "2) Parameter Estimation:\n",
        "\n",
        "* Frequentist Approach:\n",
        "\n",
        " * Parameters are estimated using sample data (e.g., means, ariances) through point estimates like maximum likelihood estimation(MLE).\n",
        "\n",
        " * Confidence intervals are used to estimate the range within which the true parameter likely lies.\n",
        "\n",
        " * Does not incorporate prior information, only the data at hand is used.\n",
        "\n",
        "* Bayesian Approach:\n",
        "\n",
        "  * Parameters are estimated as posterior distributions, derived from the prior distribution and the likelihood of the observed data.\n",
        "\n",
        "  * Provides a full distribution of the parameter, not just a point estimate.\n",
        "\n",
        "  * Prior knowledge or beliefs can influence results, and the posterior is updated as new data becomes available.\n",
        "\n",
        "3) Hypothesis Testing:\n",
        "\n",
        "* Frequentist Approach:\n",
        "\n",
        "  * Used null-hypothesis significance testing (NHST):\n",
        "   * Null hypothesis (h0): All group means are equal.\n",
        "   * Alternative hypothesis: At least one group mean is different.\n",
        "  * Produces an F-statistic and p-value. A significant p-value (<0.05) indicates sufficient evidence to reject H0.\n",
        "\n",
        "  * Binary decision-making: Reject or fail to reject H0.\n",
        "\n",
        "  * Does not provide the probability of the null hypothesis being true.\n",
        "\n",
        "* Bayesian Approach:\n",
        "\n",
        " * Hypothesis are compared using posterior probabilities or Bayes factors:\n",
        "   * Bayes factor quantifies the strength of evidence in favor of one hypothesis over another.\n",
        "\n",
        " * No strict cutoff like p<0.05; results are interpreted probabistically.\n",
        "\n",
        " * Avoids binary decision-making, focusing on the degree of evidence for completing hypothesis.\n",
        "\n",
        "4) Interpretation of results:\n",
        "\n",
        "* Frequentist Approach:\n",
        " * A significant result (e.g, p < 0.05) suggests that at least one group mean is different, but does not quantify the strength of evidence.\n",
        "\n",
        " * Confidence intervals provide a range for parameter estimates but are not directly probabilistic.\n",
        "\n",
        " * Relies on large-sample properties for validity.\n",
        "\n",
        "* Bayesian Approach:\n",
        "\n",
        "* Reults are expressed as probabilities (e.g., the probability of group A's mean being greater than Group B's means 80%).\n",
        "\n",
        "* Posterior distributions allows for direct probabilistic interpretation.\n",
        "\n",
        "* Naturally handles small samples sizes better due to the use of prior information.\n",
        "\n",
        "5) Practical Applications:\n",
        "\n",
        "* Frequentist Approach:\n",
        "\n",
        " * Widely used in traditional statistical analysis.\n",
        "\n",
        " * Preferred when there is no prior knowledge or when computational simplicity is desired.\n",
        "\n",
        " * Common in hypothesis-driven experiments.\n",
        "\n",
        "* Bayesian Approach:\n",
        "\n",
        " * Prefered when prior information is available or when a probabilistic interpretation of resulits is desired.\n",
        "\n",
        " * Useful in exploratory research, small sample studies, or when integrating multiple sources of data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R9_d2ctYzw0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8)\n",
        "\n",
        "Solution:"
      ],
      "metadata": {
        "id": "EfqUpSAYB9BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Given data\n",
        "profession_A = [48,52,55,60,62]\n",
        "profession_B = [45,50,55,52,47]\n",
        "\n",
        "# Sample variance\n",
        "var_A = np.var(profession_A, ddof=1)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "# Calculate F-statistic\n",
        "F_statistic = max(var_A, var_B) / min(var_A, var_B)\n",
        "\n",
        "# Perform the F-test and calculate p-value\n",
        "dof_A = len(profession_A) - 1\n",
        "dof_B = len(profession_B) - 1\n",
        "p_value = 2 * min(stats.f.cdf(F_statistic, dof_A, dof_B), 1 - stats.f.cdf(F_statistic, dof_A, dof_B))\n",
        "\n",
        "# Output results\n",
        "print(f\"F-statistic: {F_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "# Conclusion based on p-value\n",
        "if p_value < 0.05:\n",
        "  print(\" The variances are significantally different\")\n",
        "else:\n",
        "  print(\" The variances are not significantally different\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NibylqGRCBBi",
        "outputId": "bfc815c8-64df-4b1a-ec08-7049779ad2d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n",
            " The variances are not significantally different\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9)\n",
        "\n",
        "Solution:"
      ],
      "metadata": {
        "id": "GrMgAdGMCBnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for three regions\n",
        "region_A = [160,162,165,158,164]\n",
        "region_B = [172,175,170,168,174]\n",
        "region_C = [180,182,179,185,183]\n",
        "\n",
        "# One-way ANOVA\n",
        "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "# Results\n",
        "print(f\"F-statistic: {F_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Conclusion based on p-value\n",
        "if p_value < 0.05:\n",
        "  print(\" There is a significant difference in the means of the regions\")\n",
        "else:\n",
        "  print(\" There is no significant difference in the mean of the regions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDg7BT4ECFo5",
        "outputId": "5481f7f2-178a-422f-df1e-db1c1b1f3cc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "P-value: 2.870664187937026e-07\n",
            " There is a significant difference in the means of the regions\n"
          ]
        }
      ]
    }
  ]
}